Doing something at a few points is a basic requirement of the RPC framework, which is harder than it looks.

Let us first take a look at what the system provides: The posix system can tell the timer to trigger by [signal method](http://man7.org/linux/man-pages/man2/timer_create.2.html), but the signal is forcing We use global variables, write [async-signal-safe](https://docs.oracle.com/cd/E19455-01/806-5257/gen-26/index.html) function, in user-oriented programming In the framework, we should try our best to avoid using signal. Since 2.6.27, linux can use [fd method](http://man7.org/linux/man-pages/man2/timerfd_create.2.html) to notify the timer to trigger, this fd can be placed in epoll and transmitted data fd unified management. The only problem is: this is a system call, and we don't know how it behaves in multithreading.

Why pay so much attention to the overhead of the timer? Let us first look at how the timer is generally used in the RPC scenario:

-Set a timer in the process of initiating RPC, and cancel the waiting RPC after the timeout period. Almost all RPC calls have a timeout limit, and this timer will be set.
-Delete the timer before the end of RPC. Most RPCs are terminated by normal responses, and timers are rarely triggered.

Have you noticed that in RPC, the timer is more like an "insurance mechanism" and will not work in most cases. Naturally, we hope that its overhead is as small as possible. A function that hardly triggers requires two system calls does not seem to be ideal. How to implement timer in application framework? Talking about this issue requires a distinction between "single-threaded" and "multi-threaded":

-In a single-threaded framework, such as [libevent](http://libevent.org/)[, ](http://en.wikipedia.org/wiki/Reactor_pattern)[libev](http://software. schmorp.de/pkg/libev.html) as the eventloop library, or [GNU Pth](http://www.gnu.org/software/pth/pth-manual.html), [StateThreads](http ://state-threads.sourceforge.net/index.html) represents the coroutine/fiber class library, generally based on [small top heap](https://en.wikipedia.org/wiki/Heap_(data_structure) ) Record the trigger time. [epoll_wait](http://man7.org/linux/man-pages/man2/epoll_wait.2.html) calculates the value of the parameter timeout based on the time at the top of the heap. If there are no other events during this time, epoll_wait is also Will wake up, pop the timeout element from the heap, and call the corresponding callback function. The whole framework works like this over and over again. The creation, waiting, and deletion of timers all happen in one thread. As long as all callbacks are non-blocking and the logic is not complicated, this mechanism can provide a basically accurate timer. But as stated in [Threading Overview](threading_overview.md), this is not an RPC scenario.
-In a multi-threaded framework, any thread may be blocked by user logic for a long time. We need a separate thread to implement timer. This kind of thread is called TimerThread. A very natural approach is to use a small top heap protected by a lock. When a thread needs to create a timer, it first obtains the lock, and then inserts the corresponding time into the heap. If the inserted element becomes the earliest, it wakes up the TimerThread. The logic in TimerThread is similar to single thread, that is, waiting for the element at the top of the heap to time out. If it is inserted earlier in the waiting process, it will be awakened by the inserted thread without oversleeping. The problem with this method is that each timer needs to compete for a global lock and operate a global small top heap, as discussed repeatedly in other articles, this will trigger cache bouncing. It is normal for the same number of timer operations to be 10 times slower than the single-threaded operation. The embarrassing thing is that these timers are basically not triggered.

We focus on how to solve the problem under multi-threading.

A common idea is to hash the demand of the timer to multiple TimerThreads, but this does not work well for TimerThreads. Note that we mentioned the "constraint factor" above: once the inserted element is the earliest, the TimerThread must be awakened. Assuming that there are enough TimerThreads that each timer is hashed to an independent TimerThread, then it must wake up that TimerThread every time. "Wake up" means to trigger the linux scheduling function and trigger a context switch. In a very smooth system, this overhead is about 3-5 microseconds, which is slower than lock grab and sync cache. This factor is a difficult point in improving the scalability of TimerThread. Multiple TimerThread reduces the competitive pressure on a single small top heap, but also introduces more wake-ups.

Another difficulty is deletion. Generally use id to refer to a Timer. There are two ways to delete the Timer through this id: 1. Lock lock, find the position of the corresponding timer in the small top heap through a map, and delete it at a fixed point. This map needs to be maintained synchronously with the heap. 2. Find the memory structure of Timer by id, make a mark, and leave it to TimerThread to discover and delete by itself. The first method makes the insertion logic more complicated, the deletion also needs to grab the lock, and the thread competition is more intense. The second method leaves a lot of deleted elements in the small top heap, making the heap significantly larger, and slowing down insertion and deletion.

The third difficulty is that TimerThread should not wake up often. One extreme is that the TimerThread is always awake or wakes up at a higher frequency (such as waking up every 1ms), so that the thread that inserts the timer does not have to be responsible for waking up, and then we hash the insertion request to multiple heaps to reduce competition. The problem seems to be solved. But in fact, the timer accuracy provided by this solution is poor, generally higher than 2ms. You have to think about how to write logic for this TimerThread. It cannot wait according to the time of the top element of the heap. Because the insertion thread does not wake up, once an earlier element is inserted, the TimerThread will oversleep. The only thing it can do is sleep for a fixed time, but this conflicts with the assumption of the modern OS scheduler: the thread that sleeps frequently has the lowest priority. The result under Linux is that even if it only sleeps for a short time, it may eventually wake up for more than 2ms, because from the OS's point of view, this thread is not important. A high-precision TimerThread has a wake-up mechanism instead of periodically waking up.

In addition, more concurrent data structures are also difficult to work. Interested students can search for "concurrent priority queue" or "concurrent skip list". These data structures generally assume that the inserted values ​​are scattered, so you can modify the differences in the structure at the same time. part. But this is not true in the RPC scenario. The time set by competing threads is often concentrated in the same area, because the timeout of the program is mostly the same value, plus the current time, it is almost the same.

These factors make the design of TimerThread quite tricky. Since the qps of most users is low, which is not enough to obviously expose this scalability problem, we have been using "TimerThread protected with a lock" before r31791. TimerThread is the only high-frequency competition point of brpc under the default configuration. This problem is a technical debt that we have always been aware of. As brpc is more and more used in high qps systems, it is time to solve this problem. The TimerThread after r31791 solves the above three difficulties. The timer operation has almost no effect on the RPC performance. Let's first look at the performance difference.

> In the example program example/mutli_threaded_echo_c++, the TimerThread after r31791 is compared with the old TimerThread on the 24-core E5-2620 (hyperthreading). When sending 50 bthreads simultaneously, it saves 4% cpu (almost 1 core) and improves qps by 10 Around %; when 400 bthreads are sent synchronously, the qps rises from 300,000 to 600,000. The performance of the new TimerThread is close to that of a complete shutdown timeout.

How does the new TimerThread do it?

-One TimerThread instead of multiple.
-The created timer is hashed to multiple buckets to reduce competition between threads. The default is 13 buckets.
-Bucket does not use the small top heap management time, but the linked list + nearest_run_time field. When the inserted time is earlier than nearest_run_time, this field is overwritten, and then compared with the global nearest_run_time (different from the bucket’s nearest_run_time), if it is also earlier than this time , Modify and wake up TimerThread. Linked list nodes are allocated outside the lock using [ResourcePool](memory_management.md).
-Directly locate the timer memory structure by id when deleting, modify a flag, the timer structure is always released by TimerThread.
-After TimerThread is awakened, first set the global nearest_run_time to almost infinite (max of int64), then take out all the linked lists in the bucket, and set the nearest_run_time of the bucket to almost infinite (max of int64). TimerThread inserts the undeleted timer into the small top heap for maintenance, and this heap is used by one thread. The global nearest_run_time will be checked every time a callback is run or before sleep is prepared. If the global is earlier, it means that an earlier time has been added. Repeat this process.

The general working principle of TimerThread is outlined here. There are still many details in the project implementation. For details, please read [timer_thread.h](https://github.com/brpc/brpc/blob/master/src/bthread/timer_thread. h) and [timer_thread.cpp](https://github.com/brpc/brpc/blob/master/src/bthread/timer_thread.cpp).

The reason why this method works:

-The operation in the Bucket lock is O(1), that is, inserting a linked list node, the critical area is very small. The memory allocation of the node itself is outside the lock.
-Since most of the inserted time is incremental, there are few timers that are earlier than Bucket::nearest_run_time and participate in global competition.
-The timer that participates in the global competition is compared with the global nearest_run_time, the critical section is very small.
-Similar to Bucket, a very small number of Timers will wake up TimerThread earlier than the global nearest_run_time. Wake-up is also outside the global lock.
-Delete not participating in global competition.
-TimerThread maintains its own small top heap without any cache bouncing, which is very efficient.
-The waking frequency of TimerThread is about the inverse of RPC timeout. For example, timeout=100ms, TimerThread wakes up about 10 times per second, which is already optimal.

So far, brpc no longer has a global competition point in the default configuration. When 400 threads are running at the same time, profiling also shows that there is almost no waiting for locks.

The following are some knowledge related to time management under linux:

-The timeout accuracy of epoll_wait is milliseconds, which is poor. The timeout of pthread_cond_timedwait uses timespec, with an accuracy of nanoseconds, generally a delay of about 60 microseconds.
-For performance reasons, TimerThread uses wall-time instead of monotonic time, which may be affected by system time adjustments. Specifically, if the system time is adjusted forward or backward by one hour during the test, the program behavior will be completely undefined. In the future, users may choose monotonous time.
-On machines where the cpu supports nonstop_tsc and constant_tsc, brpc and bthread will give priority to using cpuwide_time_us based on rdtsc. Those two flags indicate that rdtsc can be used as wall-time, and the slower kernel time will be used on machines that do not support it. Most of our machines (Intel Xeon series) have those two flags. Whether rdtsc is used as wall-time will be affected by the system adjustment time, it is not clear if it has not been tested.